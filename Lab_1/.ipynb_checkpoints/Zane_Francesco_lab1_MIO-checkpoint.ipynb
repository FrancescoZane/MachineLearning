{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 1 \n",
    "Course 2022/23: F. Barbato, M. Mel, P. Zanuttigh\n",
    "\n",
    "The notebook contains some simple tasks to be performed about **classification and regression**. <br>\n",
    "Complete all the **required code sections** and **answer to all the questions**. <br>\n",
    "\n",
    "### IMPORTANT for the evaluation score:\n",
    "1. **Read carefully all cells** and **follow the instructions**\n",
    "1. **Rerun all the code from the beginning** to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebooks.\n",
    "2. Make sure to fill the code in the appropriate places **without modifying the template**, otherwise you risk breaking later cells.\n",
    "3. Please **submit the jupyter notebook file (.ipynb)**, do not submit python scripts (.py) or plain text files. **Make sure that it runs fine with the restat&run all command** - otherwise points will be deduced.\n",
    "4. **Answer the questions in the appropriate cells**, not in the ones where the question is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Classification of Day/Night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place your **name** and **ID number** (matricola) in the cell below. <br>\n",
    "Also recall to **save the file as Surname_Name_LAB1.ipynb** otherwise your homework could get lost\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name**: Francesco Zane<br>\n",
    "**ID Number**: 2076717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "The data was recorded using the new **Luxottica I-SEE glasses** in exterior conditions. These devices provide multiple **sensors mounted inside the glasses**, which can be accessed through a bluetooth connection. <br>\n",
    "For the **classification** part of the notebook we will focus on the **UVA**, **UVB** and **pressure** sensors, with the goal of discriminating between **day and night** time.\n",
    "\n",
    "![I-SEE Glasses](data/isee.png \"I-SEE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first **import** all **the packages** that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change some global settings for layout purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are in the jupyter notebook environment you can change the 'inline' option with 'notebook' to get interactive plots\n",
    "%matplotlib notebook\n",
    "# change the limit on the line length and crop to 0 very small numbers, for clearer printing\n",
    "np.set_printoptions(linewidth=500, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1) Perceptron\n",
    "In the following cells we will **implement** the **perceptron** algorithm and use it to learn a halfspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.0):** **Set** the random **seed** using your **ID**. If you need to change it for testing add a constant explicitly, eg.: 1234567 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDnumber = 2076717 # YOUR_ID\n",
    "np.random.seed(IDnumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceding to the training steps, we **load the dataset and split it** in training and test set (the **training** set is **typically larger**, here we use a 75% training 25% test split).\n",
    "The **split** is **performed after applying a random permutation** to the dataset, such permutation will **depend on the seed** you set above. Try different seeds to evaluate the impact of randomization.<br><br>\n",
    "**DO NOT CHANGE THE PRE-WRITTEN CODE UNLESS OTHERWISE SPECIFIED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dset(filename, features=[2,3,9], label_id=-1, mode='clas'):\n",
    "    # Load the dataset\n",
    "    with open(filename, newline='\\n') as f:\n",
    "        raw_data = csv.reader(f, delimiter=',')# \"lista\" di righe che vengono trasformate in liste i cui elementi\n",
    "                                               # sono gli elementi della riga divisi da virgole\n",
    "        header = next(raw_data)                # skip first line\n",
    "        print(f\"Header: {header}\\n\")\n",
    "\n",
    "        dataset = np.array(list(raw_data))     # al posto di una lista di liste creo una matrice\n",
    "        print(f\"Data shape: {dataset.shape}\\n\")# qui mi manda a schermo le dimensioni della matrice appena creata\n",
    "        print(\"Dataset Example:\")\n",
    "        #print(dataset[0,...])                  # print the first row of the dataset, same of print(dataset[0,:])\n",
    "\n",
    "    X = dataset[:,features].astype(float)      # extract, for each line, the selected features, default refers to\n",
    "                                               # the first part of the lab: [uva, uvb, pressure] and convert them \n",
    "                                               # into float\n",
    "    # in the file the last column is refered to the night(=0) or day(=1) and I want to transform these labes from 0,1\n",
    "    # to -1,1\n",
    "    if mode == 'clas':\n",
    "        Y = dataset[:,label_id].astype(int)    # if we are in classification mode, get the labels from the provided index as indices\n",
    "        Y = 2*Y-1                              # for the perceptron night --> -1, day --> 1\n",
    "    else:\n",
    "        Y = dataset[:,label_id].astype(float)  # otherwise get them as floats\n",
    "\n",
    "    m = dataset.shape[0]                       # numero di righe\n",
    "    print(\"\\nNumber of samples loaded:\", m)\n",
    "    permutation = np.random.permutation(m)     # random permutation\n",
    "\n",
    "    X = X[permutation]\n",
    "    Y = Y[permutation]\n",
    "    \n",
    "    # in this way I have permutated ramdomically the m rows\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3 -1]\n",
      " [ 4  5  6  0]\n",
      " [ 7  8  9  1]\n",
      " [10 11 12  2]\n",
      " [13 14 15  3]]\n",
      "\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]\n",
      " [13 14 15]]\n",
      "[-1  0  1  2  3]\n",
      "\n",
      "Number of samples loaded: 5\n",
      "\n",
      "[0 1 4 2 3]\n",
      "\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [13 14 15]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[-1  0  3  1  2]\n",
      "\n",
      "[[ 1.  2.  3. -1.]\n",
      " [ 4.  5.  6.  0.]\n",
      " [13. 14. 15.  3.]\n",
      " [ 7.  8.  9.  1.]\n",
      " [10. 11. 12.  2.]]\n"
     ]
    }
   ],
   "source": [
    "dataset_mine = np.array([[1,2,3,-1],\n",
    "              [4,5,6,0],\n",
    "              [7,8,9,1],\n",
    "              [10,11,12,2],\n",
    "              [13,14,15,3]])\n",
    "\n",
    "print(dataset_mine)\n",
    "\n",
    "X_mine = dataset_mine[:,:3]\n",
    "Y_mine = dataset_mine[:,-1]\n",
    "\n",
    "print()\n",
    "print(X_mine)\n",
    "print(Y_mine)\n",
    "\n",
    "\n",
    "m_mine = dataset_mine.shape[0]                       \n",
    "print(\"\\nNumber of samples loaded:\", m_mine)\n",
    "permutation_mine = np.random.permutation(m_mine)    # prende il valore m_mine e crea un vettore inserendo in modo \n",
    "                                                    # randomico tutti i valori interi x con 0 < x < m_mine-1\n",
    "\n",
    "print()\n",
    "print(permutation_mine)\n",
    "X_mine = X_mine[permutation_mine]  # quello che X_mine = X_mine[permutation_mine] fa Ã¨ \n",
    "                #(lista indici)    # X_mine[i] = X_mine[permutation_mine[i]]     per ogni riga di X\n",
    "                                   # ovvero modifica ogni riga di X mettendo al suo posto un'altra riga di X\n",
    "                                   # segundo il vettore permutation_mine   \n",
    "Y_mine = Y_mine[permutation_mine]  # stessa cosa per il vettore Y\n",
    "\n",
    "print()\n",
    "print(X_mine)\n",
    "print(Y_mine)\n",
    "\n",
    "print()\n",
    "tot = np.zeros((X_mine.shape[0],X_mine.shape[1]+1))\n",
    "for row in range(X_mine.shape[0]):\n",
    "    riga = list(X_mine[row,:])\n",
    "    riga.append(Y_mine[row])\n",
    "    tot[row,:]=riga\n",
    "\n",
    "print(tot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['rh', 'temp', 'uva', 'uvb', 'x', 'y', 'blueg', 'blueb', 'worn', 'pressure', 'timestamp', 'hour', 'minute', 'second', 'isnight']\n",
      "\n",
      "Data shape: (2177, 15)\n",
      "\n",
      "Dataset Example:\n",
      "\n",
      "Number of samples loaded: 2177\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "X, Y = load_dset('data/lux.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (day)** and **class \"-1\" (night)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in training and test sets\n",
    "\n",
    "\n",
    "\n",
    "Given $m$ total data, denote with $m_{t}$ the part used for training. Keep $m_t$ data as training data, and $m_{test}:= m-m_{t}$. <br>\n",
    "For instance one can take $m_t=0.75m$ of the data as training and $m_{test}=0.25m$ as testing. <br>\n",
    "Let us define as define\n",
    "\n",
    "$\\bullet$ $S_{t}$ the training data set\n",
    "\n",
    "$\\bullet$ $S_{test}$ the testing data set\n",
    "\n",
    "\n",
    "The reason for this splitting is as follows:\n",
    "\n",
    "TRAINING DATA: The training data are used to compute the empirical loss\n",
    "$$\n",
    "L_S(h) = \\frac{1}{m_t} \\sum_{z_i \\in S_{t}} \\ell(h,z_i)\n",
    "$$\n",
    "which is used to estimate $h$ in a given model class ${\\cal H}$.\n",
    "i.e. \n",
    "$$\n",
    "\\hat{h} = {\\rm arg\\; min}_{h \\in {\\cal H}} \\, L_S(h)\n",
    "$$\n",
    "\n",
    "TESTING DATA: The test data set can be used to estimate the performance of the final estimated model\n",
    "$\\hat h_{\\hat d_j}$ using:\n",
    "$$\n",
    "L_{{\\cal D}}(\\hat h_{\\hat d_j}) \\simeq \\frac{1}{m_{test}} \\sum_{ z_i \\in S_{test}} \\ell(\\hat h_{\\hat d_j},z_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.1):** **Divide** the **data into training and test set** (**75%** of the data in the **first** set, **25%** in the **second** one). <br>\n",
    "<br>\n",
    "Notice that as is common practice in Statistics and Machine Learning, **we scale the data** (= each variable) so that it is centered **(zero mean)** and has **standard deviation equal to 1**. <br>\n",
    "This helps in terms of numerical conditioning of the (inverse) problems of estimating the model (the coefficients of the linear regression in this case), as well as to give the same scale to all the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2177, 4)\n",
      "Number of samples in the train set: 1632\n",
      "Number of samples in the test set: 545\n",
      "\n",
      "Number of night instances in test: 281\n",
      "Number of day instances in test: 264\n",
      "\n",
      "Mean of the training input data: [1.54831112 0.44134667 0.99313922]\n",
      "Std of the training input data: [3.13159172 0.9759003  0.00391479]\n",
      "\n",
      "Mean of the test input data: [1.57227593 0.44854552 0.9929836 ]\n",
      "Std of the test input data: [2.97680353 0.93501953 0.00414219]\n"
     ]
    }
   ],
   "source": [
    "m = np.zeros((X.shape[0], X.shape[1]+1))   # riunisco X e Y in un unica matrice\n",
    "for row in range(m.shape[0]):\n",
    "    riga = list(X[row,:])\n",
    "    riga.append(Y[row])\n",
    "    m[row,:]=riga\n",
    "print(m.shape)\n",
    "\n",
    "dim_training = int(m.shape[0]*0.75//1)\n",
    "dim_test = m.shape[0]-dim_training\n",
    "\n",
    "m_training = m[:dim_training, :]\n",
    "\n",
    "m_test = m[dim_training:, :]\n",
    "\n",
    "X_training = X[:dim_training, :]\n",
    "\n",
    "Y_training = Y[:dim_training]\n",
    "\n",
    "X_test = X[dim_training:, :]\n",
    "\n",
    "Y_test = Y[dim_training:]\n",
    "\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"\\nNumber of night instances in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of day instances in test:\", np.sum(Y_test==1))\n",
    "\n",
    "\n",
    "# standardize the input matrix\n",
    "# the transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  X[:dim_training, :]\n",
    "print (\"\\nMean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\", X_training.std(axis=0))\n",
    "\n",
    "X_test =  X[dim_training:, :]\n",
    "print (\"\\nMean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **add a 1 in front of each sample** so that we can use a vector in **homogeneous coordinates** to describe all the coefficients of the model. This can be done with the function $hstack$ in $numpy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_homogeneous(X_training, X_test):\n",
    "    # Add a 1 to each sample (homogeneous coordinates)\n",
    "    X_training = np.hstack( [np.ones( (X_training.shape[0], 1) ), X_training] )\n",
    "    X_test = np.hstack( [np.ones( (X_test.shape[0], 1) ), X_test] )\n",
    "    \n",
    "    return X_training, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set in homogeneous coordinates:\n",
      "[[1.         0.0967619  0.00689714 0.99923584]\n",
      " [1.         2.65994444 0.75228444 0.99691221]\n",
      " [1.         2.09261364 0.58387818 0.9918045 ]\n",
      " [1.         0.10013462 0.00655385 0.99005637]\n",
      " [1.         0.127      0.0071     0.99332346]\n",
      " [1.         0.0635     0.         0.99773995]\n",
      " [1.         0.0635     0.00426    0.99419196]\n",
      " [1.         0.10583333 0.00355    0.99066042]\n",
      " [1.         3.67303922 1.05932    0.9912934 ]\n",
      " [1.         0.142875   0.008165   0.99317995]]\n"
     ]
    }
   ],
   "source": [
    "# convert to homogeneous coordinates using the function above\n",
    "X_training, X_test = to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])      # it's the same to write    print(X_training[ :10, : ])   first 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.2):** Now **complete** the function *perceptron*. <br>\n",
    "The **perceptron** algorithm **does not terminate** if the **data** is not **linearly separable**, therefore your implementation should **terminate** if it **reached the termination** condition seen in class **or** if a **maximum number of iterations** have already been run, where one **iteration** corresponds to **one update of the perceptron weights**. In case the **termination** is reached **because** the **maximum** number of **iterations** have been completed, the implementation should **return the best model** seen throughout .\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the perceptron\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model (or the latest, if the termination condition is reached)\n",
    "- $best\\_error$: the *fraction* of misclassified samples for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_errors(current_w, X, Y):\n",
    "    pred_lista = []\n",
    "    wrong_lista = []\n",
    "    for sample in range (X.shape[0]):\n",
    "        result = np.sign(np.inner(X[sample,:], current_w))\n",
    "        pred_lista.append(result)\n",
    "        \n",
    "        if result*Y[sample]>0:\n",
    "            wrong_lista.append(0)\n",
    "        else:\n",
    "            wrong_lista.append(True)                \n",
    "            \n",
    "    pred = np.array(pred_lista)\n",
    "    wrong = np.array(wrong_lista)\n",
    "    num_misclassified = wrong.sum()     \n",
    "    \n",
    "    if num_misclassified > 0:\n",
    "        index_misclassified = np.where(wrong)[0][0]\n",
    "    else:\n",
    "        index_misclassified = -1 \n",
    "    \n",
    "    return num_misclassified, index_misclassified   \n",
    "\n",
    "\n",
    "def perceptron_update(current_w, x, y):\n",
    "    new_w = current_w + y*x      # x is a 4-vector features associated to the wrong sample and y its label\n",
    "    return new_w\n",
    "\n",
    "\n",
    "def perceptron(X, Y, max_num_iterations):\n",
    "    \n",
    "    num_samples = X.shape[0]   \n",
    "    \n",
    "    best_error = num_samples+1\n",
    "\n",
    "    curr_w = np.zeros((1,4))\n",
    "    \n",
    "    best_w = curr_w.copy()\n",
    "    \n",
    "    num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "\n",
    "    if num_misclassified < best_error:\n",
    "        best_error = num_misclassified\n",
    "        best_w = curr_w\n",
    "    \n",
    "    num_iter = 0\n",
    "\n",
    "    while index_misclassified != -1 and num_iter < max_num_iterations:        \n",
    "        \n",
    "        curr_w = perceptron_update(curr_w, X[index_misclassified,:], Y[index_misclassified])\n",
    "\n",
    "        per = np.random.permutation(num_samples) \n",
    "        X = X[per]\n",
    "        Y = Y[per]\n",
    "        \n",
    "        num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "        \n",
    "        if num_misclassified < best_error:\n",
    "            best_error = num_misclassified\n",
    "            best_w = curr_w\n",
    "        \n",
    "        num_iter += 1\n",
    "\n",
    "    best_error = best_error/num_samples\n",
    "    \n",
    "    return best_w, best_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation above of the perceptron to learn a model from the training data using 100 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of perceptron (100 iterations): 0.032475490196078434\n",
      "Error of 3.2475490196078436%\n"
     ]
    }
   ],
   "source": [
    "# Now run the perceptron for 100 iterations\n",
    "w_found, error = perceptron(X_training,Y_training, 100)\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error))\n",
    "print(\"Error of \" + str(error*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.3):** use the best model $w\\_found$ to **predict the labels for the test dataset** and print the fraction of misclassified samples in the test set (the test error that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of perceptron (100 iterations): 0.047706422018348627\n",
      "Error of 4.770642201834862%\n"
     ]
    }
   ],
   "source": [
    "num_errors, _ = count_errors(w_found, X_test,Y_test)\n",
    "\n",
    "true_loss_estimate = num_errors/Y_test.shape[0]\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Error of \" + str(true_loss_estimate*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.Q1) [Answer the following]** <br>\n",
    "What about the difference between the training error and the test error  in terms of fraction of misclassified samples? Explain what you observe. (Notice that with a very small dataset like this one results can change due to randomization, try to run with different random seeds if you get unexpected results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q1**:<br>\n",
    "Here I can notice than, as I aspect, the value of the training error is smaller than the value of the true loss estimate. This appends becuase in the test set there are some input never seen from the algorithm during the \n",
    "    \n",
    "    \n",
    "    FINISCIIIIIIIIIIIIIIIIII\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.4):** Copy the code from the last 2 cells above in the cell below and repeat the training with 3000 iterations. Then print the error in the training set and the estimate of the true loss obtained from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of perceptron (3000 iterations): 0.029411764705882353\n",
      "Error of 2.941176470588235%\n",
      "Test Error of perceptron (3000 iterations): 0.045871559633027525\n",
      "Error of 4.587155963302752%\n"
     ]
    }
   ],
   "source": [
    "w_found, error = perceptron(X_training,Y_training, 3000) \n",
    "print(\"Training Error of perceptron (3000 iterations): \" + str(error))\n",
    "print(\"Error of \" + str(error*100) + \"%\")\n",
    "\n",
    "num_errors, _ =  count_errors(w_found, X_test,Y_test)\n",
    "\n",
    "true_loss_estimate = num_errors/Y_test.shape[0]\n",
    "print(\"Test Error of perceptron (3000 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Error of \" + str(true_loss_estimate*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.Q2) [Answer the following]** <br>\n",
    "What about the difference between the training error and the test error in terms of the fraction of misclassified samples) when running for a larger number of iterations? Explain what you observe and compare with the previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q2**:<br>\n",
    "The answers don't change very much and in both cases the error is bigger than zero. This means that the data are not perfectly linearly divisible. In this situation the algorithm will never fild a perfect solution, the best that it can do is to minimize the number of misclassified samples. The fact that with 100 iterations and with 3000 iterations we obtain similar results means that already 100 iterations are enough for the algorithm to obtain the solution that minimizes the number of misclassified samples. \n",
    "    \n",
    "    \n",
    "        CONTROLAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.2) Logistic Regression\n",
    "Now we use **logistic regression**, exploiting the implementation in **Scikit-learn**, to predict labels. We will also plot the decision boundaries of logistic regression.\n",
    "\n",
    "We first load the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['rh', 'temp', 'uva', 'uvb', 'x', 'y', 'blueg', 'blueb', 'worn', 'pressure', 'timestamp', 'hour', 'minute', 'second', 'isnight']\n",
      "\n",
      "Data shape: (2177, 15)\n",
      "\n",
      "Dataset Example:\n",
      "\n",
      "Number of samples loaded: 2177\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "X, Y = load_dset('data/lux.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.2.1):** As for the previous part, **divide the data** into training and test (75%-25%) and **add a 1 as first component** to each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (561101725.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_14596/561101725.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    m_training = # ADD YOUR CODE HERE\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# compute the splits\n",
    "m_training = # ADD YOUR CODE HERE\n",
    "\n",
    "# m_test is the number of samples in the test set (total-training)\n",
    "m_test =  # ADD YOUR CODE HERE\n",
    "\n",
    "# X_training = instances for training set\n",
    "X_training =  # ADD YOUR CODE HERE\n",
    "# Y_training = labels for the training set\n",
    "Y_training =  # ADD YOUR CODE HERE\n",
    "\n",
    "# X_test = instances for test set\n",
    "X_test =   # ADD YOUR CODE HERE\n",
    "# Y_test = labels for the test set\n",
    "Y_test =  # ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"\\nNumber of night instances in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of day instances in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# standardize the input matrix\n",
    "# the transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  # ADD YOUR CODE HERE\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test =  # ADD YOUR CODE HERE\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))\n",
    "\n",
    "# convert to homogeneous coordinates\n",
    "X_training, X_test = to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a logistic regression model in Scikit-learn use the instruction\n",
    "\n",
    "$linear\\_model.LogisticRegression(C=1e5)$\n",
    "\n",
    "($C$ is a parameter related to *regularization*, a technique that\n",
    "we will see later in the course. Setting it to a high value is almost\n",
    "as ignoring regularization, so the instruction above corresponds to the\n",
    "logistic regression you have seen in class.)\n",
    "\n",
    "To learn the model you need to use the $fit(...)$ instruction and to predict you need to use the $predict(...)$ function. <br>\n",
    "See the Scikit-learn documentation for how to use it [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "**TO DO (A.2.2):** **Define** the **logistic regression** model, then **learn** the model using **the training set** and **predict** on the **test set**. Then **print** the **fraction of samples misclassified** in the training set and in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part on logistic regression for 2 classes\n",
    "logreg = # ADD YOUR CODE HERE  # C should be very large to ignore regularization (see above)\n",
    "\n",
    "# learn from training set: hint use fit(...)\n",
    "# ADD YOUR CODE HERE\n",
    "print(\"Intercept:\" , logreg.intercept_)\n",
    "print(\"Coefficients:\" , logreg.coef_)\n",
    "\n",
    "# predict on training set\n",
    "predicted_training = # ADD YOUR CODE HERE\n",
    "\n",
    "# print the error rate = fraction of misclassified samples\n",
    "error_count_training = (predicted_training != Y_training).sum()\n",
    "error_rate_training = # ADD YOUR CODE HERE\n",
    "print(\"Error rate on training set: \"+str(error_rate_training))\n",
    "\n",
    "# predict on test set\n",
    "predicted_test = # ADD YOUR CODE HERE\n",
    "\n",
    "#print the error rate = fraction of misclassified samples\n",
    "error_count_test = (predicted_test != Y_test).sum()\n",
    "error_rate_test = # ADD YOUR CODE HERE\n",
    "print(\"Error rate on test set: \" + str(error_rate_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.2.3)** Now **pick two features** and restrict the dataset to include only two features, whose indices are specified in the $idx0$ and $idx1$ variables below. Then split into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define some additional variables:\n",
    "#    feature_names  == name to display on the plots\n",
    "#    feature_scales == scale (linear/log) to use for that metric\n",
    "# this will be referred to in the following cell - to plot the values\n",
    "feature_names  = [\"UVA\", \"UVB\", \"Atm. Pressure\"]\n",
    "feature_scales = [\"log\", \"log\", \"linear\"]\n",
    "\n",
    "# remember that we selected 3 sensors (uva, uvb, pressure), therefore\n",
    "# to make the plot we need to reduce the data to 2D, so we choose two of them\n",
    "# to do so change the following indices (between 0 and 2, inclusive)\n",
    "idx0 = # ADD YOUR CODE HERE\n",
    "idx1 = # ADD YOUR CODE HERE\n",
    "\n",
    "X_reduced = X[:,[idx0, idx1]]\n",
    "\n",
    "# re-initialize the dataset splits, with the reduced sets\n",
    "X_training = # ADD YOUR CODE HERE\n",
    "Y_training = # ADD YOUR CODE HERE\n",
    "\n",
    "X_test = # ADD YOUR CODE HERE\n",
    "Y_test = # ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now learn a model using the training data and measure the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning from training data\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# print the error rate = fraction of misclassified samples\n",
    "error_rate_test = # ADD YOUR CODE HERE\n",
    "error_rate_test = # ADD YOUR CODE HERE\n",
    "print(\"Error rate on test set: \" + str(error_rate_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.Q3) [Answer the following]** <br>\n",
    "Which features did you select and why? <br>\n",
    "Compare the perfomance of the classifiers trained with every combination of two features with that of the baseline (which used all 3 features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q3**:<br>\n",
    "answer here\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is ok, the code below uses the model in $logreg$ to plot the decision region for the two features chosen above, with colors denoting the predicted value. It also plots the points (with correct labels) in the training set. It makes a similar plot for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#firstly we import an additional module to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X_reduced[:, 0].min() - .5, X_reduced[:, 0].max() + .5\n",
    "y_min, y_max = X_reduced[:, 1].min() - .5, X_reduced[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n",
    "\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# shift all the values to positive, to avoid problems with log-log plots\n",
    "shift = .001 - X_reduced.min(axis=0)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8,3))\n",
    "axs[0].pcolormesh(xx + shift[0], yy + shift[1], Z, cmap=plt.cm.Set3)\n",
    "axs[1].pcolormesh(xx + shift[0], yy + shift[1], Z, cmap=plt.cm.Set3)\n",
    "\n",
    "# Plot also the training points\n",
    "axs[0].scatter(X_training[:, 0] + shift[0], X_training[:, 1] + shift[1], c=Y_training, edgecolors='k', cmap=plt.cm.Set3)\n",
    "\n",
    "axs[0].set_xlabel(feature_names[idx0])\n",
    "axs[0].set_ylabel(feature_names[idx1])\n",
    "axs[0].set_xscale(feature_scales[idx0])\n",
    "axs[0].set_yscale(feature_scales[idx1])\n",
    "\n",
    "axs[0].set_xlim(xx.min(), xx.max())\n",
    "axs[0].set_ylim(yy.min(), yy.max())\n",
    "axs[0].set_xticks(())\n",
    "axs[0].set_yticks(())\n",
    "axs[0].set_title('Training set')\n",
    "\n",
    "# Plot also the test points \n",
    "axs[1].scatter(X_test[:, 0] + shift[0], X_test[:, 1] + shift[1], c=Y_test, edgecolors='k', cmap=plt.cm.Set3, marker='s')\n",
    "axs[1].set_xlabel(feature_names[idx0])\n",
    "axs[1].set_ylabel(feature_names[idx1])\n",
    "axs[1].set_xscale(feature_scales[idx0])\n",
    "axs[1].set_yscale(feature_scales[idx1])\n",
    "\n",
    "axs[1].set_xlim(xx.min(), xx.max())\n",
    "axs[1].set_ylim(yy.min(), yy.max())\n",
    "axs[1].set_xticks(())\n",
    "axs[1].set_yticks(())\n",
    "axs[1].set_title('Test set')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# finally turn the warnings back on\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Linear Regression on the Atmospheric Conditions\n",
    "\n",
    "As before, these **samples** were **extracted from** a Luxottica **I-SEE** device. <br>\n",
    "For the **second part** of the laboratory we will **focus on linear regression**. We will try to estimate the **relative humidity** starting from **temperature** and **atmospheric pressure**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we load again the dataset. Notice that the indices of the features were changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, Y = load_dset('data/lux.csv', [0,1], 9, mode='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (B.1)**: split the data in training and test sets (70%-30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros((X.shape[0], X.shape[1]+1))   # riunisco X e Y in un unica matrice\n",
    "for row in range(m.shape[0]):\n",
    "    riga = list(X[row,:])\n",
    "    riga.append(Y[row])\n",
    "    m[row,:]=riga\n",
    "print(m.shape)\n",
    "\n",
    "dim_training = int(m.shape[0]*0.70//1)\n",
    "dim_test = m.shape[0]-dim_training\n",
    "\n",
    "m_training = m[:dim_training, :]\n",
    "\n",
    "m_test = m[dim_training:, :]\n",
    "\n",
    "X_training = X[:dim_training, :]\n",
    "\n",
    "Y_training = Y[:dim_training]\n",
    "\n",
    "X_test = X[dim_training:, :]\n",
    "\n",
    "Y_test = Y[dim_training:]\n",
    "\n",
    "# standardize the input matrix\n",
    "# the transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  X[:dim_training, :]\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test = X_test = X[dim_training:, :]\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training \n",
    "\n",
    "The model is trained (= estimated) minimizing the empirical error\n",
    "$$\n",
    "L_S(h) := \\frac{1}{m_t} \\sum_{z_i \\in S_{t}} \\ell(h,z_i)\n",
    "$$\n",
    "When the loss function is the quadratic loss\n",
    "$$\n",
    "\\ell(h,z) := (y - h(x))^2\n",
    "$$\n",
    "we define  the Residual Sum of Squares (RSS) as\n",
    "$$\n",
    "RSS(h):= \\sum_{z_i \\in S_{t}} \\ell(h,z_i) = \\sum_{z_i \\in S_{t}} (y_i - h(x_i))^2\n",
    "$$ so that the training error becomes\n",
    "$$\n",
    "L_S(h) = \\frac{RSS(h)}{m_t}\n",
    "$$\n",
    "\n",
    "We recal that, for linear models we have $h(x) = <w,x>$ and the Empirical error $L_S(h)$ can be written\n",
    "in terms of the vector of parameters $w$ in the form\n",
    "$$\n",
    "L_S(w) = \\frac{1}{m_t} \\|Y - X w\\|^2\n",
    "$$\n",
    "where $Y$ and $X$ are the matrices whose $i-$th row are, respectively, the output data $y_i$ and the input vectors $x_i^\\top$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (B.2):** compute the linear regression coefficients using np.linalg.lstsq from scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a 1 at the beginning of each sample for training, and testing (use homogeneous coordinates)\n",
    "X_trainingH, X_testH =  to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_trainingH[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainingH_T = X_trainingH.transpose()\n",
    "A = np.dot(X_trainingH_T, X_trainingH)\n",
    "print(type(A))\n",
    "print(A.shape)\n",
    "B = np.dot(X_trainingH_T, Y_training)\n",
    "print(type(B))\n",
    "print(B.shape)\n",
    "\n",
    "\n",
    "w_np, RSStr_np, rank_Xtr, sv_Xtr = np.linalg.lstsq(A,B, rcond=None)\n",
    "\n",
    "print(\"LS coefficients with numpy lstsq:\", w_np)\n",
    "\n",
    "Y_trainingR = Y_training\n",
    "RSStr_train = 0\n",
    "for sample in range(X_trainingH.shape[0]):\n",
    "    RSStr_train = RSStr_train + pow(Y_trainingR[sample]-np.dot(X_trainingH_T[:,sample], w_np),2)\n",
    "\n",
    "print(RSStr_train, \"    \", RSStr_np)\n",
    "\n",
    "print(\"RSS with numpy lstsq: \", RSStr_np)\n",
    "print(\"Empirical risk with numpy lstsq:\", RSStr_np/m_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainingH = np.array([[1,-1],[1,0],[1,1],[1,2]])\n",
    "Y_training = np.array([[-1],[0],[1],[2]])\n",
    "\n",
    "X_trainingH_T = X_trainingH.transpose()\n",
    "A = np.dot(X_trainingH_T, X_trainingH)\n",
    "B = np.dot(X_trainingH_T, Y_training)\n",
    "print(\"A.shape\",A.shape)\n",
    "print(\"B.shape\",B.shape)\n",
    "print(X_trainingH_T.shape)\n",
    "\n",
    "w_np, RSStr_np, rank_Xtr, sv_Xtr = np.linalg.lstsq(A,B, rcond=None)\n",
    "\n",
    "print(\"LS coefficients with numpy lstsq:\", w_np)\n",
    "print(w_np.shape)\n",
    "print(rank_Xtr)\n",
    "print(RSStr_np)\n",
    "print(RSStr_np.shape)\n",
    "\n",
    "RSStr_train = 0\n",
    "for sample in range(X_trainingH.shape[0]):\n",
    "    print(pow(Y_trainingR[sample]-np.dot(X_trainingH_T[:,sample], w_np),2))\n",
    "    RSStr_train = RSStr_train + pow(Y_trainingR[sample]-np.dot(X_trainingH_T[:,sample], w_np),2)\n",
    "print(\"My RSS error:\", RSStr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prediction \n",
    "\n",
    "Compute the output predictions on both training and validation set.and compute the Residual Sum of Sqaures (RSS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (B.3)**: Compute these quantities on  training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute predictions on training and validation\n",
    "\n",
    "#prediction_training \n",
    "prediction_training =  # ADD YOUR CODE HERE\n",
    "prediction_test =  # ADD YOUR CODE HERE\n",
    "\n",
    "#what about the loss for points in the test data?\n",
    "RSS_test =  # ADD YOUR CODE HERE\n",
    "\n",
    "print(\"RSS on test data:\",  RSS_test)\n",
    "print(\"Loss estimated from test data:\", RSS_test/m_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (B.Q1) [Answer the following]** <br>\n",
    "Comment on the results you get and on the difference between the train and test errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER B.Q1**:<br>\n",
    "answer here\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the data and check our estimation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,18))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.scatter(X_training[:,0], X_training[:,1], Y_training, label=\"Input Data\")\n",
    "ax1.scatter(X_training[:,0], X_training[:,1], prediction_training, label=\"Prediction Data\")\n",
    "ax1.set_xlabel(\"Norm. Temperature\")\n",
    "ax1.set_ylabel(\"Norm. Relative Humidity\")\n",
    "ax1.set_zlabel(\"Norm. Atm. Pressure\")\n",
    "ax1.set_title(\"Training Set\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.scatter(X_test[:,0], X_test[:,1], Y_test, label=\"Input Data\")\n",
    "ax2.scatter(X_test[:,0], X_test[:,1], prediction_test, label=\"Prediction Data\")\n",
    "ax2.set_xlabel(\"Norm. Temperature\")\n",
    "ax2.set_ylabel(\"Norm. Relative Humidity\")\n",
    "ax2.set_zlabel(\"Norm. Atm. Pressure\")\n",
    "ax2.set_title(\"Test Set\")\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least-Squares using scikit-learn\n",
    "Another fast way to compute the LS estimate is through sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinReg = linear_model.LinearRegression()  # build the object LinearRegression\n",
    "LinReg.fit(X_training, Y_training)  # estimate the LS coefficients\n",
    "print(\"Intercept:\", LinReg.intercept_)\n",
    "print(\"Least-Squares Coefficients:\", LinReg.coef_)\n",
    "prediction_training = LinReg.predict(X_training)  # predict output values on training set\n",
    "prediction_test = LinReg.predict(X_test)  # predict output values on test set\n",
    "print(\"Measure on training data:\", 1-LinReg.score(X_training, Y_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
